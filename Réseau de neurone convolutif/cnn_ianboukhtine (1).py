# -*- coding: utf-8 -*-
"""CNN-IANBOUKHTINE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KowryAPOujbRTPivfoMawyXA6Kq9VXuX
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plot
import cv2
from tensorflow.examples.tutorials.mnist import input_data



#Importation des données
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
mnist_train_images=mnist.train.images.reshape(-1,28,28,1)
mnist_train_labels=mnist.train.labels
mnist_test_images=mnist.test.images.reshape(-1,28,28,1)
mnist_test_labels=mnist.test.labels




#fonction pour obtenir des couches de Normalisation
def normalisation(couche_prec):
    mean, var=tf.nn.moments(couche_prec, [0])
    scale=tf.Variable(tf.ones(shape=(np.shape(couche_prec)[-1])))
    beta=tf.Variable(tf.zeros(shape=(np.shape(couche_prec)[-1])))
    result=tf.nn.batch_normalization(couche_prec, mean, var, beta, scale, 0.001)
    return result

#fonction de convolution 
def convolution(couche_prec, taille_noyau, nbr_noyau):
    w=tf.Variable(tf.random.truncated_normal(shape=(taille_noyau, taille_noyau, int(couche_prec.get_shape()[-1]), nbr_noyau)))
    b=np.zeros(nbr_noyau)
    result=tf.nn.conv2d(couche_prec, w, strides=[1, 1, 1, 1], padding='SAME')+b
    return result

       
def fc(couche_prec, nbr_neurone):
    w=tf.Variable(tf.random.truncated_normal(shape=(int(couche_prec.get_shape()[-1]), nbr_neurone), dtype=tf.float32))
    b=tf.Variable(np.zeros(shape=(nbr_neurone)), dtype=tf.float32)
    result=tf.matmul(couche_prec, w)+b
    return result

#Paramètres de l'apprentissage
taille_batch=100
nbr_entrainement=10
learning_rate=0.001


ph_images=tf.placeholder(shape=(None, 28, 28, 1), dtype=tf.float32, name='images')
ph_labels=tf.placeholder(shape=(None, 10), dtype=tf.float32)


#Initialisation des 2 premières couches 
result=convolution(ph_images, 5, 16)
result=normalisation(result)
result=tf.nn.relu(result)
result=convolution(result, 5, 16)
result=normalisation(result)
result=tf.nn.relu(result)
result=tf.nn.max_pool(result, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

# Initialisation des 2 dernières couches 
result=convolution(result, 5, 32)
result=normalisation(result)
result=tf.nn.relu(result)
result=convolution(result, 5, 32)
result=normalisation(result)
result=tf.nn.relu(result)
result=tf.nn.max_pool(result, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

result=tf.contrib.layers.flatten(result)

result=fc(result, 512)
result=normalisation(result)
result=tf.nn.sigmoid(result)
result=fc(result, 10)
scso=tf.nn.softmax(result,name='sortie')

loss=tf.nn.softmax_cross_entropy_with_logits_v2(labels=ph_labels, logits=result)

#2 types de méthodes d'apprentissage

#train=tf.train.AdamOptimizer(learning_rate).minimize(loss)
train=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)                

accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(scso, 1), tf.argmax(ph_labels, 1)), tf.float32))

# Instanciation de la classe sauvegarde
saver=tf.train.Saver()

with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    tab_train=[]
    tab_test=[]
    for id_entrainement in np.arange(nbr_entrainement):
        tab_accuracy_train=[]
        tab_accuracy_test=[]
        print("> Entrainement", id_entrainement)
        for batch in np.arange(0, len(mnist_train_images), taille_batch):
            s.run(train, feed_dict={
                ph_images: mnist_train_images[batch:batch+taille_batch],
                ph_labels: mnist_train_labels[batch:batch+taille_batch]
            })
        for batch in np.arange(0, len(mnist_train_images), taille_batch):
            precision=s.run(accuracy, feed_dict={
                ph_images: mnist_train_images[batch:batch+taille_batch],
                ph_labels: mnist_train_labels[batch:batch+taille_batch]
            })
            tab_accuracy_train.append(precision)
        for batch in np.arange(0, len(mnist_test_images), taille_batch):
            precision=s.run(accuracy, feed_dict={
                ph_images: mnist_test_images[batch:batch+taille_batch],
                ph_labels: mnist_test_labels[batch:batch+taille_batch]
            })
            tab_accuracy_test.append(precision)
        print("  train:", np.mean(tab_accuracy_train))
        tab_train.append(1-np.mean(tab_accuracy_train))
        print("  test :", np.mean(tab_accuracy_test))
        tab_test.append(1-np.mean(tab_accuracy_test))


	# sauvegarde du réseau
        saver.save(s,"model_mnist")

    plot.ylim(0, 1)
    plot.grid()
    plot.plot(tab_train, label="Train error")
    plot.plot(tab_test, label="Test error")
    plot.legend(loc="upper right")
    plot.show()
    resulat=s.run(scso, feed_dict={ph_images: mnist_test_images[0:taille_batch]})
    np.set_printoptions(formatter={'float': '{:0.3f}'.format})
    for image in range(taille_batch):
        print("image", image)
        print("sortie du réseau:", resulat[image], np.argmax(resulat[image]))
        print("sortie attendue :", mnist_test_labels[image], np.argmax(mnist_test_labels[image]))
        cv2_imshow(mnist_test_images[image]*255)


#La méthode DescenOptimize permet d'avoir une accuracy_test =0,98
                                   #une accury_train=0,99

#Je n'ai pas eu le temps de run complètement l'autre méthode GradientOptimizer